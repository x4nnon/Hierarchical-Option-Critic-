/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.
  logger.warn(
/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: [33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`
  logger.warn(
/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. 
  logger.deprecation(
/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(done, (bool, np.bool8)):
global_step=448, ep_r=[0.], ep_l=[57]
global_step=464, ep_r=[0.], ep_l=[59]
global_step=472, ep_r=[0.], ep_l=[60]
global_step=640, ep_r=[0.], ep_l=[81]
global_step=672, ep_r=[5.], ep_l=[85]
global_step=720, ep_r=[3.], ep_l=[91]
global_step=736, ep_r=[0.], ep_l=[93]
global_step=856, ep_r=[4.], ep_l=[108]
global_step=936, ep_r=[0.], ep_l=[58]
global_step=1000, ep_r=[5.], ep_l=[69]
global_step=1008, ep_r=[0.], ep_l=[68]
global_step=1128, ep_r=[0.], ep_l=[61]
global_step=1296, ep_r=[2.], ep_l=[78]
global_step=1400, ep_r=[0.], ep_l=[83]
global_step=1432, ep_r=[1.], ep_l=[72]
global_step=1448, ep_r=[0.], ep_l=[56]
global_step=1456, ep_r=[1.], ep_l=[92]
global_step=1528, ep_r=[0.], ep_l=[65]
global_step=1592, ep_r=[0.], ep_l=[82]
global_step=1648, ep_r=[0.], ep_l=[65]
global_step=1768, ep_r=[1.], ep_l=[59]
global_step=1896, ep_r=[0.], ep_l=[56]
global_step=2000, ep_r=[3.], ep_l=[71]
global_step=2024, ep_r=[0.], ep_l=[62]
Total loss: 5.640381813049316
global_step=2224, ep_r=[1.], ep_l=[72]
global_step=2224, ep_r=[0.], ep_l=[96]
global_step=2264, ep_r=[0.], ep_l=[84]
global_step=2344, ep_r=[0.], ep_l=[56]
global_step=2344, ep_r=[2.], ep_l=[72]
global_step=2520, ep_r=[2.], ep_l=[62]
global_step=2544, ep_r=[4.], ep_l=[143]
global_step=2768, ep_r=[0.], ep_l=[63]
global_step=2800, ep_r=[0.], ep_l=[35]
global_step=2824, ep_r=[0.], ep_l=[60]
global_step=2920, ep_r=[6.], ep_l=[115]
global_step=3064, ep_r=[2.], ep_l=[90]
global_step=3152, ep_r=[0.], ep_l=[116]
global_step=3200, ep_r=[0.], ep_l=[82]
global_step=3288, ep_r=[3.], ep_l=[133]
global_step=3304, ep_r=[1.], ep_l=[60]
global_step=3480, ep_r=[0.], ep_l=[89]
global_step=3616, ep_r=[0.], ep_l=[102]
global_step=3688, ep_r=[2.], ep_l=[78]
global_step=3792, ep_r=[0.], ep_l=[63]
global_step=3848, ep_r=[6.], ep_l=[116]
global_step=3864, ep_r=[2.], ep_l=[70]
global_step=3936, ep_r=[0.], ep_l=[57]
global_step=4016, ep_r=[0.], ep_l=[108]
Total loss: 4.058771133422852
global_step=4144, ep_r=[1.], ep_l=[118]
global_step=4168, ep_r=[2.], ep_l=[69]
global_step=4256, ep_r=[2.], ep_l=[71]
global_step=4352, ep_r=[0.], ep_l=[61]
global_step=4512, ep_r=[4.], ep_l=[83]
global_step=4600, ep_r=[0.], ep_l=[101]
global_step=4736, ep_r=[0.], ep_l=[90]
global_step=4744, ep_r=[0.], ep_l=[101]
global_step=4752, ep_r=[2.], ep_l=[73]
global_step=4840, ep_r=[0.], ep_l=[61]
global_step=4856, ep_r=[3.], ep_l=[75]
global_step=5088, ep_r=[0.], ep_l=[61]
global_step=5224, ep_r=[0.], ep_l=[59]
global_step=5256, ep_r=[0.], ep_l=[64]
global_step=5336, ep_r=[1.], ep_l=[62]
global_step=5352, ep_r=[4.], ep_l=[151]
global_step=5368, ep_r=[1.], ep_l=[107]
global_step=5512, ep_r=[3.], ep_l=[82]
global_step=5624, ep_r=[0.], ep_l=[67]
global_step=5728, ep_r=[0.], ep_l=[59]
global_step=5728, ep_r=[3.], ep_l=[124]
global_step=5784, ep_r=[0.], ep_l=[56]
global_step=5928, ep_r=[1.], ep_l=[88]
global_step=5960, ep_r=[3.], ep_l=[74]
global_step=6128, ep_r=[3.], ep_l=[77]
Total loss: 1.870840311050415
global_step=6160, ep_r=[0.], ep_l=[67]
global_step=6296, ep_r=[1.], ep_l=[64]
global_step=6344, ep_r=[0.], ep_l=[124]
global_step=6408, ep_r=[2.], ep_l=[60]
global_step=6448, ep_r=[1.], ep_l=[61]
global_step=6464, ep_r=[1.], ep_l=[92]
global_step=6520, ep_r=[0.], ep_l=[99]
global_step=6688, ep_r=[0.], ep_l=[66]
global_step=6792, ep_r=[2.], ep_l=[62]
global_step=6960, ep_r=[3.], ep_l=[69]
global_step=6984, ep_r=[0.], ep_l=[65]
global_step=7032, ep_r=[7.], ep_l=[113]
global_step=7128, ep_r=[0.], ep_l=[85]
global_step=7160, ep_r=[0.], ep_l=[46]
global_step=7264, ep_r=[1.], ep_l=[115]
global_step=7384, ep_r=[0.], ep_l=[108]
global_step=7488, ep_r=[0.], ep_l=[100]
global_step=7528, ep_r=[0.], ep_l=[68]
global_step=7568, ep_r=[0.], ep_l=[51]
global_step=7576, ep_r=[1.], ep_l=[68]
global_step=7632, ep_r=[1.], ep_l=[84]
global_step=7984, ep_r=[0.], ep_l=[107]
global_step=8000, ep_r=[0.], ep_l=[92]
global_step=8008, ep_r=[0.], ep_l=[65]
global_step=8064, ep_r=[0.], ep_l=[67]
global_step=8128, ep_r=[0.], ep_l=[93]
Total loss: 1.819042682647705
global_step=8208, ep_r=[3.], ep_l=[72]
global_step=8232, ep_r=[1.], ep_l=[82]
global_step=8392, ep_r=[3.], ep_l=[103]
global_step=8520, ep_r=[1.], ep_l=[39]
global_step=8544, ep_r=[0.], ep_l=[67]
global_step=8568, ep_r=[0.], ep_l=[63]
global_step=8592, ep_r=[4.], ep_l=[76]
global_step=8880, ep_r=[0.], ep_l=[110]
global_step=8896, ep_r=[2.], ep_l=[63]
global_step=8920, ep_r=[0.], ep_l=[99]
global_step=9040, ep_r=[0.], ep_l=[101]
global_step=9056, ep_r=[0.], ep_l=[64]
global_step=9280, ep_r=[4.], ep_l=[86]
global_step=9368, ep_r=[0.], ep_l=[100]
global_step=9416, ep_r=[1.], ep_l=[65]
global_step=9480, ep_r=[1.], ep_l=[120]
global_step=9576, ep_r=[0.], ep_l=[65]
global_step=9648, ep_r=[3.], ep_l=[76]
global_step=9760, ep_r=[1.], ep_l=[110]
global_step=9904, ep_r=[0.], ep_l=[67]
global_step=9912, ep_r=[1.], ep_l=[79]
global_step=9968, ep_r=[5.], ep_l=[69]
global_step=10104, ep_r=[2.], ep_l=[78]
global_step=10184, ep_r=[0.], ep_l=[76]
Total loss: 1.7001979351043701
global_step=10264, ep_r=[2.], ep_l=[77]
global_step=10264, ep_r=[3.], ep_l=[168]
global_step=10544, ep_r=[2.], ep_l=[79]
global_step=10592, ep_r=[0.], ep_l=[86]
global_step=10696, ep_r=[0.], ep_l=[64]
global_step=10728, ep_r=[4.], ep_l=[95]
global_step=10888, ep_r=[3.], ep_l=[141]
global_step=10896, ep_r=[4.], ep_l=[79]
global_step=11040, ep_r=[0.], ep_l=[97]
global_step=11048, ep_r=[0.], ep_l=[57]
global_step=11088, ep_r=[0.], ep_l=[123]
global_step=11168, ep_r=[0.], ep_l=[55]
global_step=11168, ep_r=[2.], ep_l=[78]
global_step=11232, ep_r=[0.], ep_l=[67]
global_step=11472, ep_r=[5.], ep_l=[72]
global_step=11520, ep_r=[0.], ep_l=[59]
global_step=11664, ep_r=[0.], ep_l=[72]
global_step=11792, ep_r=[4.], ep_l=[78]
global_step=11896, ep_r=[5.], ep_l=[91]
global_step=11920, ep_r=[0.], ep_l=[110]
global_step=12032, ep_r=[2.], ep_l=[70]
global_step=12048, ep_r=[0.], ep_l=[102]
Total loss: 1.1682590246200562
global_step=12296, ep_r=[3.], ep_l=[176]
global_step=12320, ep_r=[0.], ep_l=[100]
global_step=12368, ep_r=[0.], ep_l=[72]
global_step=12568, ep_r=[1.], ep_l=[113]
global_step=12584, ep_r=[0.], ep_l=[67]
global_step=12624, ep_r=[2.], ep_l=[74]
global_step=12672, ep_r=[4.], ep_l=[97]
global_step=12776, ep_r=[0.], ep_l=[107]
global_step=13016, ep_r=[0.], ep_l=[87]
global_step=13032, ep_r=[0.], ep_l=[83]
global_step=13120, ep_r=[0.], ep_l=[67]
global_step=13160, ep_r=[0.], ep_l=[108]
global_step=13224, ep_r=[1.], ep_l=[69]
global_step=13416, ep_r=[2.], ep_l=[106]
global_step=13504, ep_r=[3.], ep_l=[110]
global_step=13504, ep_r=[0.], ep_l=[91]
global_step=13552, ep_r=[0.], ep_l=[67]
global_step=13640, ep_r=[0.], ep_l=[76]
global_step=13728, ep_r=[4.], ep_l=[63]
global_step=13808, ep_r=[2.], ep_l=[49]
global_step=13944, ep_r=[0.], ep_l=[103]
global_step=14080, ep_r=[2.], ep_l=[115]
global_step=14088, ep_r=[3.], ep_l=[73]
global_step=14296, ep_r=[5.], ep_l=[71]
global_step=14328, ep_r=[1.], ep_l=[65]
Total loss: 1.0079858303070068
global_step=14344, ep_r=[0.], ep_l=[99]
global_step=14464, ep_r=[0.], ep_l=[120]
global_step=14480, ep_r=[0.], ep_l=[67]
global_step=14592, ep_r=[5.], ep_l=[119]
global_step=14808, ep_r=[0.], ep_l=[60]
global_step=14848, ep_r=[0.], ep_l=[63]
global_step=14888, ep_r=[2.], ep_l=[74]
global_step=14984, ep_r=[0.], ep_l=[63]
global_step=15024, ep_r=[0.], ep_l=[118]
global_step=15024, ep_r=[2.], ep_l=[117]
global_step=15192, ep_r=[0.], ep_l=[91]
global_step=15208, ep_r=[1.], ep_l=[77]
global_step=15336, ep_r=[1.], ep_l=[66]
global_step=15384, ep_r=[0.], ep_l=[67]
global_step=15680, ep_r=[0.], ep_l=[87]
global_step=15768, ep_r=[0.], ep_l=[93]
global_step=15808, ep_r=[4.], ep_l=[75]
global_step=15816, ep_r=[4.], ep_l=[99]
global_step=15824, ep_r=[4.], ep_l=[117]
global_step=15912, ep_r=[0.], ep_l=[66]
global_step=16104, ep_r=[0.], ep_l=[114]
global_step=16192, ep_r=[0.], ep_l=[64]
global_step=16320, ep_r=[5.], ep_l=[62]
Total loss: 0.598810613155365
global_step=16416, ep_r=[0.], ep_l=[135]
global_step=16416, ep_r=[3.], ep_l=[76]
global_step=16448, ep_r=[0.], ep_l=[67]
global_step=16512, ep_r=[0.], ep_l=[93]
global_step=16672, ep_r=[0.], ep_l=[44]
global_step=16752, ep_r=[5.], ep_l=[117]
global_step=16872, ep_r=[0.], ep_l=[96]
global_step=16888, ep_r=[0.], ep_l=[87]
global_step=17016, ep_r=[4.], ep_l=[75]
global_step=17144, ep_r=[0.], ep_l=[87]
global_step=17200, ep_r=[6.], ep_l=[66]
global_step=17232, ep_r=[0.], ep_l=[90]
global_step=17392, ep_r=[3.], ep_l=[80]
global_step=17424, ep_r=[0.], ep_l=[67]
global_step=17576, ep_r=[2.], ep_l=[145]
global_step=17632, ep_r=[4.], ep_l=[77]
global_step=17672, ep_r=[0.], ep_l=[59]
global_step=17672, ep_r=[1.], ep_l=[100]
global_step=17680, ep_r=[0.], ep_l=[67]
global_step=17936, ep_r=[0.], ep_l=[64]
global_step=17992, ep_r=[0.], ep_l=[95]
global_step=18016, ep_r=[4.], ep_l=[78]
global_step=18256, ep_r=[1.], ep_l=[85]
global_step=18328, ep_r=[2.], ep_l=[82]
global_step=18376, ep_r=[0.], ep_l=[87]
Total loss: 0.9051039814949036
global_step=18472, ep_r=[0.], ep_l=[67]
global_step=18496, ep_r=[0.], ep_l=[30]
global_step=18576, ep_r=[2.], ep_l=[70]
global_step=18640, ep_r=[0.], ep_l=[121]
global_step=18664, ep_r=[0.], ep_l=[42]
global_step=18768, ep_r=[0.], ep_l=[97]
global_step=18816, ep_r=[0.], ep_l=[40]
global_step=18896, ep_r=[0.], ep_l=[65]
global_step=19008, ep_r=[0.], ep_l=[67]
global_step=19104, ep_r=[8.], ep_l=[184]
global_step=19168, ep_r=[4.], ep_l=[74]
global_step=19408, ep_r=[0.], ep_l=[96]
global_step=19456, ep_r=[4.], ep_l=[99]
global_step=19480, ep_r=[1.], ep_l=[83]
global_step=19544, ep_r=[0.], ep_l=[97]
global_step=19544, ep_r=[0.], ep_l=[67]
global_step=19592, ep_r=[0.], ep_l=[87]
global_step=19712, ep_r=[5.], ep_l=[76]
global_step=19832, ep_r=[1.], ep_l=[83]
global_step=20080, ep_r=[0.], ep_l=[67]
global_step=20112, ep_r=[0.], ep_l=[82]
global_step=20128, ep_r=[0.], ep_l=[67]
global_step=20160, ep_r=[0.], ep_l=[85]
global_step=20208, ep_r=[0.], ep_l=[100]
global_step=20248, ep_r=[0.], ep_l=[88]
global_step=20296, ep_r=[5.], ep_l=[73]
Total loss: 0.4409444332122803
global_step=20480, ep_r=[1.], ep_l=[81]
global_step=20584, ep_r=[0.], ep_l=[57]
global_step=20584, ep_r=[0.], ep_l=[63]
global_step=20776, ep_r=[1.], ep_l=[83]
global_step=20800, ep_r=[2.], ep_l=[63]
global_step=20840, ep_r=[2.], ep_l=[85]
global_step=20984, ep_r=[0.], ep_l=[92]
global_step=20984, ep_r=[0.], ep_l=[97]
global_step=21104, ep_r=[0.], ep_l=[65]
global_step=21120, ep_r=[0.], ep_l=[67]
global_step=21136, ep_r=[1.], ep_l=[82]
global_step=21136, ep_r=[0.], ep_l=[37]
global_step=21264, ep_r=[1.], ep_l=[61]
global_step=21344, ep_r=[2.], ep_l=[68]
global_step=21608, ep_r=[0.], ep_l=[63]
global_step=21656, ep_r=[0.], ep_l=[67]
global_step=21688, ep_r=[0.], ep_l=[88]
global_step=21752, ep_r=[0.], ep_l=[96]
global_step=21816, ep_r=[0.], ep_l=[85]
global_step=21856, ep_r=[2.], ep_l=[64]
global_step=21968, ep_r=[5.], ep_l=[104]
global_step=22000, ep_r=[9.], ep_l=[92]
global_step=22112, ep_r=[0.], ep_l=[63]
global_step=22184, ep_r=[0.], ep_l=[66]
global_step=22360, ep_r=[2.], ep_l=[68]
global_step=22448, ep_r=[0.], ep_l=[95]
global_step=22456, ep_r=[4.], ep_l=[75]
global_step=22496, ep_r=[0.], ep_l=[93]
Total loss: 0.6284041404724121
global_step=22624, ep_r=[0.], ep_l=[64]
global_step=22808, ep_r=[4.], ep_l=[101]
global_step=22880, ep_r=[0.], ep_l=[87]
global_step=22888, ep_r=[2.], ep_l=[66]
global_step=22912, ep_r=[3.], ep_l=[118]
global_step=23016, ep_r=[2.], ep_l=[70]
global_step=23128, ep_r=[0.], ep_l=[63]
global_step=23208, ep_r=[0.], ep_l=[95]
global_step=23256, ep_r=[0.], ep_l=[56]
global_step=23288, ep_r=[0.], ep_l=[99]
global_step=23416, ep_r=[0.], ep_l=[67]
global_step=23440, ep_r=[3.], ep_l=[69]
global_step=23552, ep_r=[1.], ep_l=[80]
global_step=23624, ep_r=[4.], ep_l=[76]
global_step=23632, ep_r=[0.], ep_l=[63]
global_step=23712, ep_r=[2.], ep_l=[57]
global_step=23952, ep_r=[0.], ep_l=[67]
global_step=23984, ep_r=[3.], ep_l=[68]
global_step=23984, ep_r=[0.], ep_l=[97]
global_step=24056, ep_r=[0.], ep_l=[96]
global_step=24128, ep_r=[0.], ep_l=[62]
global_step=24192, ep_r=[2.], ep_l=[71]
global_step=24232, ep_r=[0.], ep_l=[65]
global_step=24368, ep_r=[1.], ep_l=[102]
global_step=24448, ep_r=[0.], ep_l=[62]
global_step=24536, ep_r=[3.], ep_l=[69]
Total loss: 0.5796528458595276
global_step=24632, ep_r=[0.], ep_l=[63]
global_step=24736, ep_r=[0.], ep_l=[94]
global_step=24752, ep_r=[0.], ep_l=[27]
global_step=24808, ep_r=[1.], ep_l=[77]
global_step=24968, ep_r=[0.], ep_l=[27]
global_step=24976, ep_r=[0.], ep_l=[66]
global_step=24984, ep_r=[5.], ep_l=[77]
global_step=25016, ep_r=[4.], ep_l=[98]
global_step=25016, ep_r=[3.], ep_l=[120]
global_step=25136, ep_r=[0.], ep_l=[63]
global_step=25384, ep_r=[5.], ep_l=[72]
global_step=25464, ep_r=[0.], ep_l=[91]
global_step=25488, ep_r=[0.], ep_l=[59]
global_step=25512, ep_r=[0.], ep_l=[67]
global_step=25584, ep_r=[4.], ep_l=[75]
global_step=25656, ep_r=[0.], ep_l=[65]
global_step=25800, ep_r=[0.], ep_l=[98]
global_step=25896, ep_r=[1.], ep_l=[116]
global_step=25960, ep_r=[5.], ep_l=[72]
global_step=26032, ep_r=[0.], ep_l=[65]
global_step=26192, ep_r=[1.], ep_l=[76]
global_step=26192, ep_r=[0.], ep_l=[67]
global_step=26240, ep_r=[0.], ep_l=[97]
global_step=26304, ep_r=[4.], ep_l=[102]
global_step=26544, ep_r=[5.], ep_l=[73]
global_step=26568, ep_r=[0.], ep_l=[84]
global_step=26592, ep_r=[0.], ep_l=[99]
Total loss: 0.6265289783477783
global_step=26728, ep_r=[0.], ep_l=[87]
global_step=26728, ep_r=[0.], ep_l=[67]
global_step=26792, ep_r=[5.], ep_l=[75]
global_step=26816, ep_r=[3.], ep_l=[64]
global_step=26840, ep_r=[0.], ep_l=[34]
global_step=27096, ep_r=[2.], ep_l=[69]
global_step=27136, ep_r=[5.], ep_l=[112]
global_step=27224, ep_r=[0.], ep_l=[62]
global_step=27336, ep_r=[0.], ep_l=[93]
global_step=27344, ep_r=[4.], ep_l=[66]
global_step=27360, ep_r=[0.], ep_l=[65]
global_step=27424, ep_r=[1.], ep_l=[79]
global_step=27424, ep_r=[0.], ep_l=[87]
global_step=27632, ep_r=[3.], ep_l=[67]
global_step=27720, ep_r=[0.], ep_l=[62]
Traceback (most recent call last):
  File "/home/x4nno/Documents/PhD/HOC/methods/HOC.py", line 581, in <module>
    main_training_loop(agent, args, writer, envs, device)
  File "/home/x4nno/Documents/PhD/HOC/methods/HOC.py", line 537, in main_training_loop
    meta_option_termination_probs = agent.termination_function_meta(next_obs, current_meta_options)
  File "/home/x4nno/Documents/PhD/HOC/OC_agents/HOC_agent.py", line 265, in termination_function_meta
    termination_prob = self.meta_termination[i](state_rep[mask])
  File "/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/x4nno/Documents/PhD/HOC/OC_agents/HOC_agent.py", line 135, in forward
    termination_prob = torch.sigmoid(self.T(x))
  File "/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/x4nno/anaconda3/envs/fracos_ppo/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt