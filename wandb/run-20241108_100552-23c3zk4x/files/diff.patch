diff --git a/methods/HOC.py b/methods/HOC.py
index 743b721..e75fba3 100644
--- a/methods/HOC.py
+++ b/methods/HOC.py
@@ -64,8 +64,8 @@ class Args:
     wandb_entity: str = "tpcannon"
     capture_video: bool = False
     env_id: str = "procgen-starpilot"
-    total_timesteps: int = 1000000
-    learning_rate: float = 3e-4
+    total_timesteps: int = 20000000
+    learning_rate: float = 5e-4
     num_envs: int = 8
     num_steps: int = 256
     anneal_lr: bool = True
@@ -75,10 +75,10 @@ class Args:
     report_epoch: int = 81920
     anneal_ent: bool = True
     # entropy coefficients
-    action_ent_coef: float = 0.005
-    option_ent_coef: float = 0.005
-    meta_ent_coef: float = 0.005
-    max_grad_norm: float = 0.1
+    action_ent_coef: float = 0.015
+    option_ent_coef: float = 0.1
+    meta_ent_coef: float = 0.5
+    max_grad_norm: float = 0.5
     batch_size: int = 0
     minibatch_size: int = 0
     num_iterations: int = 0
@@ -228,241 +228,6 @@ def compute_returns_and_advantages(rewards, values, dones, next_value, gamma=0.9
     return returns, advantages
 
 
-def oc(args):
-    args.batch_size = int(args.num_envs * args.num_steps)
-    args.minibatch_size = int(args.batch_size // args.num_minibatches)
-    args.num_iterations = args.total_timesteps // args.batch_size
-    run_name = f"OC_{args.env_id}__{args.exp_name}__{args.seed}__{datetime.now()}"
-
-    if args.track and not args.debug:
-        import wandb
-        wandb.init(project=args.wandb_project_name, entity=args.wandb_entity, sync_tensorboard=True, config=vars(args), name=run_name, monitor_gym=True, save_code=True)
-
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text("hyperparameters", "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])))
-
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    envs = SyncVectorEnv([make_env(args.env_id, i, args.capture_video, run_name, args, sl=args.proc_start, nl=args.proc_num_levels, easy=args.easy) for i in range(args.num_envs)])
-    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
-
-    # Initialize Option-Critic agent
-    agent = HOCAgent(
-        input_channels=3, 
-        num_actions=envs.single_action_space.n, 
-        num_options=4,  # Number of options for OC agent
-        hidden_dim=256,
-        gamma=args.gamma,
-        learning_rate=args.learning_rate
-    ).to(device)
-    
-    
-    ####### load if not in warmup
-    if not args.warmup:
-        state_rep_dict = torch.load(f"OC_policies/{args.env_id}/termination.pth")
-        agent.conv1.load_state_dict(state_rep_dict['conv1'])
-        agent.conv2.load_state_dict(state_rep_dict['conv2'])
-        agent.conv3.load_state_dict(state_rep_dict['conv3'])
-        agent.fc1.load_state_dict(state_rep_dict['fc1'])
-        agent.termination.load_state_dict(state_rep_dict['termination'])
-        
-        agent.intra_option_policy.load_state_dict(torch.load(f"OC_policies/{args.env_id}/intra.pth"))
-
-    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-    options_buffer = torch.zeros((args.num_steps, args.num_envs), dtype=torch.long).to(device)  # Track active options
-    actions_buffer = torch.zeros((args.num_steps, args.num_envs), dtype=torch.long).to(device)  # Track primitive actions
-    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-    global_step_truth = 0
-    next_obs, _ = envs.reset(seed=args.seed)
-    next_obs = torch.Tensor(next_obs).to(device)
-    next_done = torch.zeros(args.num_envs).to(device)
-
-    current_options = torch.full((args.num_envs,), -1, dtype=torch.long, device=device)
-    for iteration in range(1, args.num_iterations + 1):
-        if args.anneal_lr:
-            frac = 1.0 - (global_step_truth / args.total_timesteps)
-            optimizer.param_groups[0]["lr"] = frac * args.learning_rate
-            
-        if args.anneal_ent:
-            frac = 1.0 - (global_step_truth / args.total_timesteps)
-            ent_action_coef_now = frac * args.ent_coef_action
-            ent_option_coef_now = frac * args.ent_coef_option
-        else:
-            ent_action_coef_now = args.ent_coef_action
-            ent_option_coef_now = args.ent_coef_option
-            
-
-        for step in range(0, args.num_steps):
-            obs[step] = next_obs
-            dones[step] = next_done
-        
-            with torch.no_grad():
-                # For environments where the option has terminated (or no active option), we select a new option
-                needs_new_option = current_options == -1  # Determine which environments need a new option
-                
-                if needs_new_option.any():
-                    # Select new options for environments where needed
-                    new_options, _ = agent.select_option(next_obs[needs_new_option])
-                    current_options[needs_new_option] = new_options  # Update current options
-        
-                option_mask = current_options != -1  # Mask for environments where an option is active
-                current_actions = torch.full((args.num_envs,), -1, dtype=torch.long).to(device)  # Placeholder for actions
-        
-                # Select actions using the intra-option policy for all environments (since all have active options now)
-                intra_option_actions = agent.select_action(next_obs, current_options)
-                current_actions = intra_option_actions  # Assign the actions for all environments
-                
-                _, value = agent.compute_q_value(next_obs, current_options, current_actions)
-                values[step] = value
-        
-            # Compute the Q-value (value) for the current state, for all environments
-            
-        
-            # Store the active options and primitive actions in the buffer for later updates
-            options_buffer[step] = current_options  # Store the option that was active during this step
-            actions_buffer[step] = current_actions  # Store the primitive action taken during this step
-            
-        
-            # Step the environment with the selected primitive actions (for all environments in batch)
-            next_obs, reward, terminations, truncations, infos = envs.step(current_actions.cpu().numpy())  # Actions are now valid for all envs
-            next_done = np.logical_or(terminations, truncations)
-            rewards[step] = torch.tensor(reward).to(device).reshape(-1)
-            
-            if args.debug:
-                plot_all_procgen_obs(next_obs, envs, current_options, current_actions)
-            
-            if reward.any():
-                pass # debug point
-        
-            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)
-            
-            if "final_info" in infos:
-                ref=0
-                for info in infos["final_info"]:
-                    if info and ("episode" in info):
-                        print(f"global_step={global_step_truth}, ep_r={info['episode']['r']}, ep_l={info['episode']['l']}")
-                        writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step_truth)
-                        writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step_truth)
-                        # plot_specific_procgen_obs(next_obs, envs, ref)
-                    ref += 1
-        
-            # Option Termination Check
-            with torch.no_grad():
-                option_mask = current_options != -1  # Mask for environments where an option is active
-                if option_mask.any():
-                    # Only compute termination probabilities for environments where options are active
-                    termination_probs = agent.termination_function(next_obs[option_mask], current_options[option_mask])
-                    terminated = torch.bernoulli(termination_probs).bool()  # Sample termination decisions
-        
-                    # Set current_options to -1 for environments where the option terminated
-                    old_current_options = current_options.clone()
-                    current_options[option_mask] = torch.where(terminated, torch.tensor(-1, device=device, dtype=torch.long), current_options[option_mask])        
-            global_step_truth += args.num_envs
-
-        for epoch in range(args.update_epochs):
-    
-            # Compute the next value for the final step
-            with torch.no_grad():
-                _, next_value = agent.compute_q_value(next_obs, old_current_options, current_actions)
-        
-            # Initialize advantage buffers
-            option_advantages = torch.zeros_like(rewards).to(device)  # For policy over options
-            action_advantages = torch.zeros_like(rewards).to(device)  # For intra-option policies
-        
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-        
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t + 1]
-                    nextvalues = values[t + 1]
-        
-                # Delta for GAE (action advantages)
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                action_advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-        
-                # Compute option advantages
-
-                q_values_for_options = agent.compute_q_values_for_all_options(obs[t])  # Get Q-values for all options at time t
-                q_value_for_past_option = torch.gather(q_values_for_options, 1, options_buffer[t].reshape(-1, 1)).squeeze(1)
-    
-                if obs[t].shape[-1] == 3:
-                    state = obs[t].permute(0, 3, 1, 2) 
-    
-                option_logits = agent.policy_over_options(state / 255)  # Get option logits at time t
-                option_probs = Categorical(logits=option_logits)  # Softmax to convert logits to probabilities
-                V_state = torch.sum(option_probs.probs * q_values_for_options, dim=-1)  # V(s), the value function over all options
-    
-                # Option advantage: A(s, o) = Q(s, o) - V(s)
-                option_advantages[t] = q_value_for_past_option - V_state  # Use the value for the active option
-        
-            # Compute action returns using the action advantages
-            returns = action_advantages + values  # Action returns based on action advantages
-        
-            # Flatten the batch for updating the policies
-            b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-            b_actions = actions_buffer.reshape((-1,) + envs.single_action_space.shape)
-            b_options = options_buffer.reshape((-1,))  # Flattened options
-            b_action_advantages = action_advantages.reshape(-1)
-            b_option_advantages = option_advantages.reshape(-1)
-            b_returns = returns.reshape(-1)
-            b_values = values.reshape(-1)
-        
-            # ------------------- Perform updates -------------------
-            
-            _, newvalues = agent.compute_q_value(b_obs, b_options, b_actions)
-
-            agent.update_combined_all(b_obs, b_options, b_actions, b_option_advantages,
-                                      b_action_advantages, newvalues, b_returns, global_step_truth,
-                                      writer, args, ent_action_coef_now, ent_option_coef_now)
-            
-        print("update complete")
-        
-        torch.cuda.empty_cache()
-        if global_step_truth >= args.total_timesteps:
-            if not args.warmup:
-                conduct_evals(agent, writer, global_step_truth, run_name, device)
-                    
-    
-    if args.warmup:
-        
-        # check make folder
-        folder_path = f"OC_policies/{args.env_id}"
-        if not os.path.exists(folder_path):
-            os.makedirs(folder_path)
-            print(f"Folder '{folder_path}' created.")
-        else:
-            print(f"Folder '{folder_path}' already exists.")
-        
-        # save the intra_option
-        torch.save(agent.intra_option_policy.state_dict(), f"OC_policies/{args.env_id}/intra.pth")
-        
-        # save the state_rep and termination:
-        state_rep_dict = {
-            'conv1': agent.conv1.state_dict(),
-            'conv2': agent.conv2.state_dict(),
-            'conv3': agent.conv3.state_dict(),
-            'fc1': agent.fc1.state_dict(),
-            'termination': agent.termination.state_dict(),
-        }
-        torch.save(state_rep_dict, f"OC_policies/{args.env_id}/termination.pth")
-        
-        
-    envs.close()
-    writer.close()
-
 
 def main_training_loop(agent, args, writer, envs, device):
     global_step_truth = 0
@@ -484,6 +249,20 @@ def main_training_loop(agent, args, writer, envs, device):
     current_meta_options = torch.full((args.num_envs,), -1, dtype=torch.long, device=device)
 
     for iteration in range(1, args.num_iterations + 1):
+        # Anneal learning rate
+        if args.anneal_lr:
+            frac = 1.0 - (iteration - 1.0) / args.num_iterations
+            lr = args.learning_rate * frac
+            for param_group in agent.optimizer.param_groups:
+                param_group['lr'] = lr
+
+        # Anneal entropy coefficients
+        if args.anneal_ent:
+            frac = 1.0 - (iteration - 1.0) / args.num_iterations
+            agent.action_ent_coef = args.action_ent_coef * frac
+            agent.option_ent_coef = args.option_ent_coef * frac
+            agent.meta_ent_coef = args.meta_ent_coef * frac
+
         for step in range(0, args.num_steps):
             obs[step] = next_obs
             dones[step] = next_done
@@ -571,7 +350,7 @@ if __name__ == "__main__":
     agent = HOCAgent(
             input_channels=3,
             num_meta_options=4,
-            num_options=16,
+            num_options=4,
             num_actions=envs.single_action_space.n, 
             hidden_dim=256,
             gamma=args.gamma,
diff --git a/utils/__pycache__/sync_vector_env.cpython-39.pyc b/utils/__pycache__/sync_vector_env.cpython-39.pyc
index 8c0597e..8e148c4 100644
Binary files a/utils/__pycache__/sync_vector_env.cpython-39.pyc and b/utils/__pycache__/sync_vector_env.cpython-39.pyc differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 6ca9f4e..1dddaed 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20241107_174956-4dr49y5l/logs/debug-internal.log
\ No newline at end of file
+run-20241108_100552-23c3zk4x/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index b5ce18a..ad8fe5d 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20241107_174956-4dr49y5l/logs/debug.log
\ No newline at end of file
+run-20241108_100552-23c3zk4x/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 8315e40..4b889bb 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20241107_174956-4dr49y5l
\ No newline at end of file
+run-20241108_100552-23c3zk4x
\ No newline at end of file
